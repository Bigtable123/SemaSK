{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import ast\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from haversine import haversine, Unit\n",
    "from math import radians, cos\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from haversine import haversine, Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your own openai api key \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"\n",
    "# Get this api key from https://www.mapbox.com/\n",
    "geo_api_key = \"your_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may change the path of this two csv files\n",
    "df = pd.read_csv('business.csv')\n",
    "df_tip = pd.read_csv('tip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the name of the city you want to process\n",
    "selected_city = 'Santa Barbara'\n",
    "df_city = df[df['city'] == selected_city]\n",
    "df_city.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_preprocessed = df_tip.groupby('business_id')['text'].agg(list).reset_index()\n",
    "merged_df = pd.merge(df_city, tip_preprocessed, on='business_id', how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_df[merged_df['text'].apply(lambda x: isinstance(x, list) and len(x) > 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_city))\n",
    "print(len(merged_df))\n",
    "print(len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are a master of summarising reviews, now I have some tips, they are in the form of lists in python and split with comma, I would like you to help me make a summary for each lists. Here are some example of summary:\n",
    "list:  [\\'Love their pastries and drinks!\\', \\'Really good egg tart and bubble tea.  Best we have had in Chinatown so far\\', \\'Was told they stopped making almond tarts... Unfortunately that was one of their best items I looked forward to when visiting Philly.\\', \\'After 6 pm the bread is on sale!\\', \\'Best steamed pork bun in chinatown!\\', \\'great cupcakes & almond cookies\\', \"Order the rainbow cake it\\'s pretty and not too sweet\", \\'bun is sucked here and the waitress was really mean and cheap\\', \\'I found chilled pork buns that I could take home and steam. They turned out great.\\', \\'The cold tea w milk makes my day.\\']\n",
    "summary: Customers praise this establishment for its excellent pastries and drinks, highlighting the egg tart, bubble tea, steamed pork bun, cupcakes, almond cookies, and a particularly pretty, not too sweet rainbow cake. The best experiences include finding high-quality items like chilled pork buns for home steaming and enjoying discounts on bread after 6 pm. However, there's disappointment over the discontinuation of almond tarts and negative feedback about the quality of buns and customer service from the staff.\n",
    "list:['Love sonic but orders are constantly wrong...', 'Foods always been good. Shakes r delicious!']\n",
    "summary: The feedback highlights a mix of experiences at Sonic. While there is love for the brand and appreciation for the quality of food and delicious shakes, there is also frustration over frequent inaccuracies in order fulfillment.\n",
    "now it is your turn.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"list: {input}, summary:\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\")\n",
    "chain = prompt_template | llm | output_parser\n",
    "\n",
    "def summarize_tip(tip):\n",
    "    response = chain.invoke({\"input\":tip})\n",
    "    return response\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    chunk['tips_summary'] = chunk['text'].apply(summarize_tip)\n",
    "    return chunk\n",
    "\n",
    "\n",
    "\n",
    "chunk_size = 200 \n",
    "\n",
    "start_chunk_index = 0\n",
    "chunks = [merged_df [i:i+chunk_size] for i in range(0, merged_df .shape[0], chunk_size)]\n",
    "processed_chunks = []\n",
    "\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "    processed_chunk = process_chunk(chunk)\n",
    "    processed_chunks.append(processed_chunk)\n",
    "    processed_chunk.to_csv(\"summary_n.csv\", mode='a', header=not bool(i), index=False)\n",
    "\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P = pd.read_csv('summary_n.csv')\n",
    "df_P.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_address(lat, lon,api = geo_api_key):\n",
    "    url = f\"https://api.mapbox.com/search/geocode/v6/reverse?longitude={lon}&latitude={lat}&limit=1&access_token={api}\"  \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        data = response.json()\n",
    "        \n",
    "\n",
    "        if not data.get('features'):\n",
    "            return {\"error\": \"No features found in the API response.\"}\n",
    "        \n",
    "\n",
    "        feature = data['features'][0]\n",
    "        properties = feature.get('properties', {})\n",
    "        context = properties.get('context', {})\n",
    "\n",
    "        neighborhood = None\n",
    "        postcode = None\n",
    "        address = None\n",
    "\n",
    "        neighborhood = context.get('neighborhood', {}).get('name')\n",
    "        postcode = context.get('postcode', {}).get('name')\n",
    "        address = context.get('address', {}).get('name')\n",
    "\n",
    "\n",
    "        if not neighborhood:\n",
    "            neighborhood = properties.get('neighborhood')\n",
    "\n",
    "        return {\n",
    "            'neighborhood': neighborhood,\n",
    "            'postcode': postcode,\n",
    "            'full_address':address\n",
    "        }\n",
    "        \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        return {\"error\": f\"HTTP error occurred: {http_err}\"}\n",
    "    except Exception as err:\n",
    "        return {\"error\": f\"Other error: {err}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 200\n",
    "num_chunks = int(np.ceil(len(df_P) / chunk_size))\n",
    "\n",
    "# Create a directory for temporary results\n",
    "temp_dir = Path(\"./yelp_temp_results_n\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start_chunk_index = 6\n",
    "\n",
    "for i in range(start_chunk_index, num_chunks):\n",
    "    start_index = i * chunk_size\n",
    "    end_index = (i + 1) * chunk_size\n",
    "    \n",
    "    # Retrieve the current chunk\n",
    "    chunk = df_P.iloc[start_index:end_index]\n",
    "    \n",
    "    # Apply the function to each row in the chunk and expand the results into new columns\n",
    "    chunk[['neighbourhood', 'postcode','full_address']] = chunk.apply(\n",
    "        lambda row: pd.Series(complete_address(row['latitude'], row['longitude'])),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Save the processed chunk to a file\n",
    "    temp_file = temp_dir / f\"chunk_{i}.csv\"\n",
    "    chunk.to_csv(temp_file, index=False)\n",
    "    \n",
    "    print(f\"Chunk {i} processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = Path(\"./yelp_temp_results_n\")\n",
    "\n",
    "chunks_paths = list(temp_dir.glob(\"chunk_*.csv\"))\n",
    "\n",
    "if not chunks_paths:\n",
    "    raise FileNotFoundError(\"No chunk files found in 'temp_results' directory.\")\n",
    "\n",
    "all_chunks = [pd.read_csv(chunk_path) for chunk_path in sorted(chunks_paths, key=lambda path: int(path.stem.split('_')[-1]))]\n",
    "merged_results_loc = pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results_loc['description'] =  merged_results_loc.apply(lambda row: (\n",
    "    f\"{row['name']} is located at {row['full_address']} and primarily serves the category of {row['categories']}. \"\n",
    "    f\"It is open for business at these hours: {row['hours']}. \"\n",
    "    f\"Customers often highlight: '{row['tips_summary']}'.\"\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_results_loc[merged_results_loc['text'].str.len() > 20]\n",
    "filtered_df = filtered_df.dropna(subset=['full_address'])\n",
    "filtered_df = filtered_df.dropna(subset=['hours'])\n",
    "\n",
    "final_input1 = filtered_df[['business_id','name','latitude','longitude','description']]\n",
    "final_input2 = filtered_df[['business_id','name','longitude','latitude','full_address','categories','stars','tips_summary','description']]\n",
    "\n",
    "\n",
    "print(len(merged_results_loc))\n",
    "print(len(filtered_df))\n",
    "print(len(final_input1))\n",
    "print(len(final_input2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_formatted = selected_city.replace(\" \", \"_\")\n",
    "file_path1 = f\"input/yelp_test_input_{city_formatted}.csv\"\n",
    "file_path2 = f\"input/yelp_demo_input_{city_formatted}.csv\"\n",
    "file_path3 = f\"yelp_full_{city_formatted}.csv\"\n",
    "\n",
    "final_input1.to_csv(file_path1,index=False)\n",
    "final_input2.to_csv(file_path2,index=False)\n",
    "filtered_df.to_scv(file_path3,index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
